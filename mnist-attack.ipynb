{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.utils.data as DataUtils\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Readymade data loading function\n",
    "DATA_ROOT='./MNISTData/'\n",
    "def getMNISTDataLoaders(batchSize=64, nTrain=50000, nVal=10000, nTest=10000):\n",
    "  # You can use technically use the same transform instance for all 3 sets\n",
    "  assert (60000 - nVal) == nTrain, 'nTrain + nVal must be equal to 60000'\n",
    "  trainTransform = transforms.Compose([transforms.ToTensor()])\n",
    "  valTransform = transforms.Compose([transforms.ToTensor()])\n",
    "  testTransform = transforms.Compose([transforms.ToTensor()])\n",
    "  \n",
    "  trainSet = datasets.MNIST(root=DATA_ROOT, download=True, train=True, \\\n",
    "                           transform=trainTransform)\n",
    "  valSet = datasets.MNIST(root=DATA_ROOT, download=True, train=True, \\\n",
    "                         transform=valTransform)\n",
    "  testSet = datasets.MNIST(root=DATA_ROOT, download=True, train=False, \\\n",
    "                                 transform=testTransform)\n",
    "  \n",
    "  indices = np.arange(0, 60000)\n",
    "  np.random.shuffle(indices)\n",
    "  \n",
    "  trainSampler = SubsetRandomSampler(indices[:nTrain])\n",
    "  valSampler = SubsetRandomSampler(indices[nTrain:])\n",
    "  testSampler = SubsetRandomSampler(np.arange(0, nTest))\n",
    "  \n",
    "  trainLoader = DataUtils.DataLoader(trainSet, batch_size=batchSize, \\\n",
    "                                   sampler=trainSampler)\n",
    "  valLoader = DataUtils.DataLoader(valSet, batch_size=batchSize, \\\n",
    "                                  sampler=valSampler)\n",
    "  testLoader = DataUtils.DataLoader(testSet, batch_size=batchSize, \\\n",
    "                                    sampler=testSampler)\n",
    "  return trainLoader, valLoader, testLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "                \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook will use PyTorch Device: CUDA\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Notebook will use PyTorch Device: ' + device.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Progress Bar Function\n",
    "def progress(curr, total, suffix=''):\n",
    "  bar_len = 48\n",
    "  filled = int(round(bar_len * curr / float(total)))\n",
    "  if filled == 0:\n",
    "    filled = 1\n",
    "  bar = '=' * (filled - 1) + '>' + '-' * (bar_len - filled)\n",
    "  sys.stdout.write('\\r[%s] .. %s' % (bar, suffix))\n",
    "  sys.stdout.flush()\n",
    "  if curr == total:\n",
    "    bar = bar_len * '='\n",
    "    sys.stdout.write('\\r[%s] .. %s .. Completed\\n' % (bar, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "n_epochs = 1\n",
    "lr = 1e-2\n",
    "step = 0\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNISTData/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:01, 9136025.22it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNISTData/MNIST/raw/train-images-idx3-ubyte.gz to ./MNISTData/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNISTData/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 88371.31it/s]            \n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNISTData/MNIST/raw/train-labels-idx1-ubyte.gz to ./MNISTData/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNISTData/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 2320725.21it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNISTData/MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNISTData/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNISTData/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 29756.34it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNISTData/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNISTData/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(n_epochs):\\n  for j, (images, labels) in enumerate(train_loader):\\n    images, labels = images.to(device), labels.to(device)\\n    optimizer.zero_grad()\\n    logits = model(images)\\n    loss = criterion(logits, labels)\\n    loss.backward()\\n    optimizer.step()\\n    if j % 8 == 0:\\n      progress(j+1, len(train_loader), 'Batch [{}/{}] Epoch [{}/{}] Loss = {:.3f}'.format(j+1, len(train_loader), i+1, n_epochs, loss.item()))\\n    step += 1\\nend_time = time.time()\\nprint('\\nTotal training steps = {}'.format(step))\\nprint('Total time taken = {}'.format(end_time - start_time))\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = getMNISTDataLoaders()\n",
    "start_time = time.time()\n",
    "\"\"\"\n",
    "for i in range(n_epochs):\n",
    "  for j, (images, labels) in enumerate(train_loader):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(images)\n",
    "    loss = criterion(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if j % 8 == 0:\n",
    "      progress(j+1, len(train_loader), 'Batch [{}/{}] Epoch [{}/{}] Loss = {:.3f}'.format(j+1, len(train_loader), i+1, n_epochs, loss.item()))\n",
    "    step += 1\n",
    "end_time = time.time()\n",
    "print('\\nTotal training steps = {}'.format(step))\n",
    "print('Total time taken = {}'.format(end_time - start_time))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install advertorch > /dev/null\n",
    "import advertorch\n",
    "print(advertorch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Documentation for this attack can be found at the link below\\n# https://advertorch.readthedocs.io/en/latest/advertorch/attacks.html#advertorch.attacks.GradientSignAttack\\nadversary_1 = GradientSignAttack(model, eps=0.3)\\ncorrect = 0\\nmodel.eval()\\nfor j, (images, labels) in enumerate(test_loader):\\n  images, labels = images.to(device), labels.to(device)\\n  adv_images_1 = adversary_1.perturb(images, labels) # This is extra step as compared to normal clean accuracy testing\\n  logits = model(adv_images_1)\\n  _, preds = torch.max(logits, 1)\\n  correct += (preds == labels).sum().item()\\n  progress(j+1, len(test_loader), 'Batch [{}/{}]'.format(j+1, len(test_loader)))\\nmodel.train()\\nprint('Accuracy on FGSM adversarial samples = {}%'.format(float(correct) * 100 / 10000))\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Evaluating against FGSM attack\n",
    "from advertorch.attacks import GradientSignAttack\n",
    "\"\"\"\n",
    "# Documentation for this attack can be found at the link below\n",
    "# https://advertorch.readthedocs.io/en/latest/advertorch/attacks.html#advertorch.attacks.GradientSignAttack\n",
    "adversary_1 = GradientSignAttack(model, eps=0.3)\n",
    "correct = 0\n",
    "model.eval()\n",
    "for j, (images, labels) in enumerate(test_loader):\n",
    "  images, labels = images.to(device), labels.to(device)\n",
    "  adv_images_1 = adversary_1.perturb(images, labels) # This is extra step as compared to normal clean accuracy testing\n",
    "  logits = model(adv_images_1)\n",
    "  _, preds = torch.max(logits, 1)\n",
    "  correct += (preds == labels).sum().item()\n",
    "  progress(j+1, len(test_loader), 'Batch [{}/{}]'.format(j+1, len(test_loader)))\n",
    "model.train()\n",
    "print('Accuracy on FGSM adversarial samples = {}%'.format(float(correct) * 100 / 10000))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Documentation for this attack can be found at the link below\\n# https://advertorch.readthedocs.io/en/latest/advertorch/attacks.html#advertorch.attacks.GradientSignAttack\\nadversary_2 = LinfBasicIterativeAttack(model, eps=0.1 , nb_iter=40)\\ncorrect = 0\\nmodel.eval()\\nfor j, (images, labels) in enumerate(test_loader):\\n  images, labels = images.to(device), labels.to(device)\\n  adv_images_2 = adversary_2.perturb(images, labels) # This is extra step as compared to normal clean accuracy testing\\n  logits = model(adv_images_2)\\n  _, preds = torch.max(logits, 1)\\n  correct += (preds == labels).sum().item()\\n  progress(j+1, len(test_loader), 'Batch [{}/{}]'.format(j+1, len(test_loader)))\\nmodel.train()\\nprint('Accuracy on FGSM adversarial samples = {}%'.format(float(correct) * 100 / 10000))\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Evaluating against IFSGM attack\n",
    "from advertorch.attacks import LinfBasicIterativeAttack\n",
    "\"\"\"\n",
    "# Documentation for this attack can be found at the link below\n",
    "# https://advertorch.readthedocs.io/en/latest/advertorch/attacks.html#advertorch.attacks.GradientSignAttack\n",
    "adversary_2 = LinfBasicIterativeAttack(model, eps=0.1 , nb_iter=40)\n",
    "correct = 0\n",
    "model.eval()\n",
    "for j, (images, labels) in enumerate(test_loader):\n",
    "  images, labels = images.to(device), labels.to(device)\n",
    "  adv_images_2 = adversary_2.perturb(images, labels) # This is extra step as compared to normal clean accuracy testing\n",
    "  logits = model(adv_images_2)\n",
    "  _, preds = torch.max(logits, 1)\n",
    "  correct += (preds == labels).sum().item()\n",
    "  progress(j+1, len(test_loader), 'Batch [{}/{}]'.format(j+1, len(test_loader)))\n",
    "model.train()\n",
    "print('Accuracy on FGSM adversarial samples = {}%'.format(float(correct) * 100 / 10000))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Documentation for this attack can be found at the link below\\n# https://advertorch.readthedocs.io/en/latest/advertorch/attacks.html#advertorch.attacks.GradientSignAttack\\nadversary_3 = LinfPGDAttack(model, eps=0.3 , nb_iter = 40)\\ncorrect = 0\\nmodel.eval()\\nfor j, (images, labels) in enumerate(test_loader):\\n  images, labels = images.to(device), labels.to(device)\\n  adv_images_3 = adversary_3.perturb(images, labels) # This is extra step as compared to normal clean accuracy testing\\n  logits = model(adv_images_3)\\n  _, preds = torch.max(logits, 1)\\n  correct += (preds == labels).sum().item()\\n  progress(j+1, len(test_loader), 'Batch [{}/{}]'.format(j+1, len(test_loader)))\\nmodel.train()\\nprint('Accuracy on FGSM adversarial samples = {}%'.format(float(correct) * 100 / 10000))\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Evaluating against PGD attack\n",
    "from advertorch.attacks import LinfPGDAttack\n",
    "\"\"\"\n",
    "# Documentation for this attack can be found at the link below\n",
    "# https://advertorch.readthedocs.io/en/latest/advertorch/attacks.html#advertorch.attacks.GradientSignAttack\n",
    "adversary_3 = LinfPGDAttack(model, eps=0.3 , nb_iter = 40)\n",
    "correct = 0\n",
    "model.eval()\n",
    "for j, (images, labels) in enumerate(test_loader):\n",
    "  images, labels = images.to(device), labels.to(device)\n",
    "  adv_images_3 = adversary_3.perturb(images, labels) # This is extra step as compared to normal clean accuracy testing\n",
    "  logits = model(adv_images_3)\n",
    "  _, preds = torch.max(logits, 1)\n",
    "  correct += (preds == labels).sum().item()\n",
    "  progress(j+1, len(test_loader), 'Batch [{}/{}]'.format(j+1, len(test_loader)))\n",
    "model.train()\n",
    "print('Accuracy on FGSM adversarial samples = {}%'.format(float(correct) * 100 / 10000))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 70\n",
    "lr = 1e-2\n",
    "step = 0\n",
    "xent_loss = nn.CrossEntropyLoss()\n",
    "adv_model = model\n",
    "adv_model.train()\n",
    "optimizer = torch.optim.SGD(adv_model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "train_loader, val_loader, test_loader = getMNISTDataLoaders()\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============================================>] .. Batch [777/782] Epoch [70/70] Loss = 0.040\n",
      "Total training steps = 54740\n",
      "Total time taken = 16093.067623853683\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Although not officially mentioned, making `size_average=False` for the loss \n",
    "function improves reliability of the result in PyTorch 0.4.0. This is required\n",
    "since we are taking step against the gradient for \"every\" image in the batch.\n",
    "So reducing them to a single value won't cut it.\n",
    "\"\"\"\n",
    "#training on FSGM\n",
    "advertorch_loss_fn = nn.CrossEntropyLoss(size_average=False)\n",
    "for i in range(n_epochs):\n",
    "  for j, (images, labels) in enumerate(train_loader):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \"\"\"\n",
    "    Creating the adversary :\n",
    "    ------------------------\n",
    "    Adversarial examples should be typically generated when model parameters are not \n",
    "    changing i.e. model parameters are frozen. This step may not be required for very\n",
    "    simple linear models, but is a must for models using components such as dropout \n",
    "    or batch normalization.\n",
    "    \"\"\"\n",
    "    adv_model.eval() # Freezes the model parameters\n",
    "    \"\"\"\n",
    "    The `clip` values here determine the clipping range after taking the adversarial step\n",
    "    The clipping is essential to keep the domain of input images within the range\n",
    "    MNIST images for this notebook are normalized to [0, 1]. If you're using something else, \n",
    "    make sure to modify these values accordingly. The `eps` value decides the magnitude\n",
    "    of the attack. For all MNIST models, the threat model advises to stick to maximum eps of 0.3 \n",
    "    for input in range [0, 1]\n",
    "    \"\"\"\n",
    "    fgsm_adversary = GradientSignAttack(adv_model, advertorch_loss_fn, eps=0.3, clip_min=0., \\\n",
    "                    clip_max=1., targeted=False)\n",
    "    adv_images = fgsm_adversary.perturb(images, labels) # Generate adversarial samples\n",
    "    ifgsm_adversary = LinfBasicIterativeAttack(adv_model, advertorch_loss_fn, eps=0.1, clip_min=0., \\\n",
    "                    clip_max=1., targeted=False , nb_iter=40)\n",
    "    adv_images_2 = ifgsm_adversary.perturb(images, labels) # Generate adversarial samples\n",
    "    pgd_adversary = LinfPGDAttack(adv_model, advertorch_loss_fn, eps=0.3, clip_min=0., \\\n",
    "                    clip_max=1., targeted=False , nb_iter = 40)\n",
    "    adv_images_3 = pgd_adversary.perturb(images, labels)\n",
    "    adv_model.train() # Allows model parameters to be changed again\n",
    "    optimizer.zero_grad()\n",
    "    logits = adv_model(images)\n",
    "    loss = criterion(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_images = adv_images \n",
    "    train_labels = labels\n",
    "    optimizer.zero_grad()\n",
    "    logits = adv_model(train_images)\n",
    "    loss = xent_loss(logits, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_images = adv_images_2 \n",
    "    train_labels = labels\n",
    "    optimizer.zero_grad()\n",
    "    logits = adv_model(train_images)\n",
    "    loss = xent_loss(logits, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_images = adv_images_3 \n",
    "    train_labels = labels\n",
    "    optimizer.zero_grad()\n",
    "    logits = adv_model(train_images)\n",
    "    loss = xent_loss(logits, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if j % 8 == 0:\n",
    "      progress(j+1, len(train_loader), 'Batch [{}/{}] Epoch [{}/{}] Loss = {:.3f}'.format(j+1, len(train_loader), i+1, n_epochs, loss.item()))\n",
    "    step += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "print('\\nTotal training steps = {}'.format(step))\n",
    "print('Total time taken = {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================] .. Batch [157/157] .. Completed\n",
      "Accuracy = 99.35%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct = 0\n",
    "model.eval()\n",
    "for j, (images, labels) in enumerate(test_loader):\n",
    "  images, labels = images.to(device), labels.to(device)\n",
    "  logits = model(images)\n",
    "  _, preds = torch.max(logits, 1)\n",
    "  correct += (preds == labels).sum().item()\n",
    "  progress(j+1, len(test_loader), 'Batch [{}/{}]'.format(j+1, len(test_loader)))\n",
    "model.train()\n",
    "print('Accuracy = {}%'.format(float(correct) * 100 / 10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================] .. Batch [157/157] .. Completed\n",
      "Accuracy on FGSM adversarial samples = 96.81%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating against FGSM attack\n",
    "\n",
    "from advertorch.attacks import GradientSignAttack\n",
    "# Documentation for this attack can be found at the link below\n",
    "# https://advertorch.readthedocs.io/en/latest/advertorch/attacks.html#advertorch.attacks.GradientSignAttack\n",
    "adversary_1 = GradientSignAttack(adv_model, eps=0.3)\n",
    "correct = 0\n",
    "adv_model.eval()\n",
    "for j, (images, labels) in enumerate(test_loader):\n",
    "  images, labels = images.to(device), labels.to(device)\n",
    "  adv_images_1 = adversary_1.perturb(images, labels)\n",
    "  logits = adv_model(adv_images_1)\n",
    "  _, preds = torch.max(logits, 1)\n",
    "  correct += (preds == labels).sum().item()\n",
    "  progress(j+1, len(test_loader), 'Batch [{}/{}]'.format(j+1, len(test_loader)))\n",
    "adv_model.train()\n",
    "print('Accuracy on FGSM adversarial samples = {}%'.format(float(correct) * 100 / 10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================] .. Batch [157/157] .. Completed\n",
      "Accuracy on FGSM adversarial samples = 98.45%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating against I-FGSM attack\n",
    "\n",
    "from advertorch.attacks import LinfBasicIterativeAttack\n",
    "# Documentation for this attack can be found at the link below\n",
    "# https://advertorch.readthedocs.io/en/latest/advertorch/attacks.html#advertorch.attacks.GradientSignAttack\n",
    "adversary_2 = LinfBasicIterativeAttack(adv_model, eps=0.1 , nb_iter=40)\n",
    "correct = 0\n",
    "adv_model.eval()\n",
    "for j, (images, labels) in enumerate(test_loader):\n",
    "  images, labels = images.to(device), labels.to(device)\n",
    "  adv_images_2 = adversary_2.perturb(images, labels)\n",
    "  logits = adv_model(adv_images_2)\n",
    "  _, preds = torch.max(logits, 1)\n",
    "  correct += (preds == labels).sum().item()\n",
    "  progress(j+1, len(test_loader), 'Batch [{}/{}]'.format(j+1, len(test_loader)))\n",
    "adv_model.train()\n",
    "print('Accuracy on FGSM adversarial samples = {}%'.format(float(correct) * 100 / 10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================] .. Batch [157/157] .. Completed\n",
      "Accuracy on FGSM adversarial samples = 95.13%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating against PGD attack\n",
    "\n",
    "from advertorch.attacks import LinfPGDAttack\n",
    "# Documentation for this attack can be found at the link below\n",
    "# https://advertorch.readthedocs.io/en/latest/advertorch/attacks.html#advertorch.attacks.GradientSignAttack\n",
    "adversary_3 = LinfPGDAttack(adv_model, eps=0.3)\n",
    "correct = 0\n",
    "adv_model.eval()\n",
    "for j, (images, labels) in enumerate(test_loader):\n",
    "  images, labels = images.to(device), labels.to(device)\n",
    "  adv_images_3 = adversary_3.perturb(images, labels)\n",
    "  logits = adv_model(adv_images_3)\n",
    "  _, preds = torch.max(logits, 1)\n",
    "  correct += (preds == labels).sum().item()\n",
    "  progress(j+1, len(test_loader), 'Batch [{}/{}]'.format(j+1, len(test_loader)))\n",
    "adv_model.train()\n",
    "print('Accuracy on FGSM adversarial samples = {}%'.format(float(correct) * 100 / 10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(adv_model.state_dict(), 'model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
